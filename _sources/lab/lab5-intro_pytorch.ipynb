{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mjwen/chee6397/blob/main/lab/lab5-intro_pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Intro to automatic differentiation and PyTorch  \n",
    "\n",
    "This notebook is intended to introduce:\n",
    "- the basic ideas of automatic differentiation and how to use it in PyTorch;\n",
    "- the basic ideas of PyTorch and how to use it to train a simple MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Follow the instructions on the [PyTorch website](https://pytorch.org/get-started/locally/) to install PyTorch. \n",
    "We will only use the CPU version in this lab, so you can ignore the CUDA related options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch can be used as a replacement of Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1 = np.array([[1., 2., 3.], [4., 5., 6.]])\n",
    "M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.array([1., 2., 3.]).reshape(3, 1)\n",
    "\n",
    "v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M1@v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2 = torch.tensor([[1., 2., 3.], [4., 5., 6.]])\n",
    "M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v2 = torch.tensor([1., 2., 3.]).reshape(3, 1)\n",
    "\n",
    "M2@v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting between numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M2.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following simple example to illustrate the basic ideas of automatic differentiation.\n",
    "\n",
    "$L = (y - wx)^2$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = 2(y - wx)(-x)$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x} = 2(y - wx)(-w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.])\n",
    "w = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "y = torch.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff =  y - w*x\n",
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square = diff**2\n",
    "square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = torch.sum(square)\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the gradient of L with respect to w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw = torch.autograd.grad(L, w)\n",
    "\n",
    "# dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cannot get grad of L with respect to x (because x does not require grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dx = torch.autograd.grad(L, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative way to get grad instead of calling autograd.grad\n",
    "\n",
    "Note:\n",
    "- this is the recommended way to get the gradient.\n",
    "- cannot backpropagate a second time. So comment out the above dw cells to run the below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can even get higher order derivatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(w):\n",
    "    x = torch.tensor([1., 2., 3.])\n",
    "    y = torch.zeros(3)\n",
    "\n",
    "    L = torch.sum((y - w*x)**2)\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Hessian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd.functional import hessian\n",
    "\n",
    "w = torch.tensor([4., 5., 6.], requires_grad=True)\n",
    "\n",
    "hessian(get_loss, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyTwoLayerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, num_input_nodes, num_hidden_nodes, num_out_nodes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_hidden_nodes = num_hidden_nodes\n",
    "        self.num_out_nodes = num_out_nodes\n",
    "\n",
    "        self.linear1 = nn.Linear(num_input_nodes, num_hidden_nodes)\n",
    "        self.linear2 = nn.Linear(num_hidden_nodes, num_out_nodes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        h = self.linear1(X)\n",
    "        h = torch.relu(h)\n",
    "        y = self.linear2(h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create some data to test our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # random seed\n",
    "\n",
    "def generate_data(N, num_in=3):\n",
    "    \"\"\"Generate some random data with num_in=3 features x1, x2, x3.\n",
    "    The target is y = x1^2 + x2^2 + x3^2.\n",
    "\n",
    "    Args:\n",
    "        N: number of samples\n",
    "        num_in: number of input features\n",
    "    \"\"\"\n",
    "    X = torch.randn(N, num_in)\n",
    "    y = torch.square(X).sum(dim=1).reshape(N, 1)\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X, y = generate_data(N=100, num_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a model with 10 hidden nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_in = 3\n",
    "num_hidden = 10\n",
    "num_out  = 1\n",
    "\n",
    "model = MyTwoLayerMLP(num_in, num_hidden, num_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X)\n",
    "\n",
    "y_pred.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_step(model, optimizer, X, y):\n",
    "    \"\"\"Performs one step of gradient descent on the given model.\n",
    "\n",
    "    Args:\n",
    "        model: the model to train\n",
    "        optimizer: the optimizer to use\n",
    "        X: the input data\n",
    "        y: the target data\n",
    "    \"\"\"\n",
    "    y_pred = model(X)\n",
    "    loss = torch.mean((y_pred - y)**2)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using SGD optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 30\n",
    "\n",
    "losses = []\n",
    "for s in range(num_steps):\n",
    "    for X_i, y_i in zip(X, y):\n",
    "        l = train_one_step(model, optimizer, X_i, y_i)\n",
    "        losses.append(l.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(losses)\n",
    "\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# Note, the loss is the training loss for each individual sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
